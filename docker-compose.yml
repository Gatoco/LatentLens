# ====================================================================
# Docker Compose para LatentLens - Versión de Desarrollo Robusta
# ====================================================================
# Este archivo define la orquesta completa de nuestra aplicación,
# usando "bind mounts" explícitos para asegurar la sincronización
# de archivos entre el host (tu PC) y los contenedores.
# ====================================================================

services:

  # --------------------------------------------------------------------
  # Servicio 1: La API de Recomendación
  # --------------------------------------------------------------------
  api:
    # Construye la imagen para este servicio usando el Dockerfile
    # que se encuentra en el directorio actual (.).
    build: .

    # Mapea el puerto 8000 de tu PC al puerto 8000 del contenedor.
    ports:
      - "8000:8000"

    # Mapea el directorio './mlruns' de tu PC al directorio '/app/mlruns'
    # dentro de este contenedor. La variable ${PWD} es sustituida por
    # Docker Compose por la ruta absoluta del proyecto, lo que resuelve
    # problemas de ambigüedad en Windows.
    volumes:
      - ${PWD}/mlruns:/app/mlruns

    # Asigna un nombre de red predecible al contenedor.
    hostname: latentlens-api

  # --------------------------------------------------------------------
  # Servicio 2: La Interfaz de Usuario de MLflow
  # --------------------------------------------------------------------
  mlflow-ui:
    # Usa la imagen oficial de MLflow que es compatible con la versión
    # de la librería que estamos usando en nuestro notebook.
    image: ghcr.io/mlflow/mlflow:v3.2.0

    # Mapea el puerto 5000 de tu PC al puerto 5000 del contenedor.
    ports:
      - "5000:5000"

    # Mapea el MISMO directorio './mlruns' de tu PC al directorio '/mlruns'
    # dentro de este contenedor. Ambos servicios ahora ven la misma carpeta.
    volumes:
      - ${PWD}/mlruns:/mlruns

    # El comando que se ejecuta al iniciar el contenedor para lanzar la UI,
    # asegurando que escuche en todas las interfaces y sepa dónde
    # buscar los archivos de experimentos.
    command: mlflow ui --host 0.0.0.0 --backend-store-uri /mlruns
