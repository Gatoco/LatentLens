# ğŸ¯ Q1-M2: Sistema de RecomendaciÃ³n - Sprint de ProfundizaciÃ³n (Mes 2)

**Objetivo del Mes:** Evolucionar de un modelo simple a un sistema hÃ­brido, mejorar significativamente las mÃ©tricas de evaluaciÃ³n y enriquecer la API para manejar escenarios del mundo real.

---

## ğŸ“Œ Semana 5: MÃ©tricas de Ranking y Contenido

*   [ ] **MÃ©tricas:** Dejar de usar solo RMSE. Investigar e implementar mÃ©tricas de ranking que son mÃ¡s apropiadas para la recomendaciÃ³n.
    *   [ ] Escribir una funciÃ³n para calcular **Precision@k** y **Recall@k**.
    *   [ ] (Bonus) Investigar y tratar de implementar **MAP@k** (Mean Average Precision at k).
*   [ ] **MLflow:** Registrar las nuevas mÃ©tricas de ranking para tus modelos existentes (Baseline y SVD) en MLflow. Ahora podrÃ¡s comparar no solo el error, sino la calidad del ranking.
*   [ ] **Modelo (Basado en Contenido):** Iniciar la parte de contenido del sistema.
    *   [ ] Usar los datos de `movies.csv`. Combinar el `title` y los `genres`.
    *   [ ] Crear una matriz de caracterÃ­sticas de texto usando **TF-IDF Vectorizer** de Scikit-learn sobre los gÃ©neros/tÃ­tulos.
    *   [ ] Calcular la similitud del coseno entre todas las pelÃ­culas para encontrar las mÃ¡s parecidas.
*   [ ] **API:** Crear un nuevo endpoint `/similar/{movie_id}` que devuelva las 10 pelÃ­culas mÃ¡s similares basadas en contenido (usando la matriz de similitud del coseno).
*   [ ] **Retrospectiva Semanal:** Anotar: Â¿Por quÃ© las mÃ©tricas de ranking son mÃ¡s Ãºtiles aquÃ­? Â¿QuÃ© limitaciones tiene el enfoque basado en contenido?

---

## ğŸ“Œ Semana 6: Modelo HÃ­brido y el Problema del "Cold Start"

*   [ ] **Modelo (HÃ­brido):** DiseÃ±ar e implementar una estrategia para combinar tu modelo de filtrado colaborativo (SVD) con tu modelo basado en contenido.
    *   **Estrategia simple:** Para un usuario dado, genera 20 recomendaciones con SVD y 20 con contenido (basado en las pelÃ­culas que le han gustado). Combina y re-rankea los resultados.
*   [ ] **"Cold Start":** Manejar explÃ­citamente el problema del "arranque en frÃ­o".
    *   [ ] **Nuevos Usuarios:** Â¿QuÃ© le recomiendas a un usuario sin calificaciones? Implementa una lÃ³gica en la API que, si el usuario es nuevo, le devuelva las pelÃ­culas mÃ¡s populares o mejor calificadas.
    *   [ ] **Nuevas PelÃ­culas:** AsegÃºrate de que tu sistema no ignore pelÃ­culas que aÃºn no han recibido calificaciones (el modelo basado en contenido ayuda aquÃ­).
*   [ ] **CÃ³digo:** Refactorizar la lÃ³gica de recomendaciÃ³n en `/src` para que sea mÃ¡s limpia. Crea una clase `Recommender` que encapsule los diferentes modelos.
*   [ ] **MLflow:** Registrar el rendimiento (con todas las mÃ©tricas) de tu nuevo modelo hÃ­brido. Â¿Supera a los modelos individuales?
*   [ ] **Retrospectiva Semanal:** Anotar: Â¿QuÃ© estrategia de hibridaciÃ³n usaste y por quÃ©? Â¿QuÃ© otras estrategias existen?

---

## ğŸ“Œ Semana 7: ConfiguraciÃ³n, Pruebas y API Avanzada

*   [ ] **ConfiguraciÃ³n:** Externalizar las configuraciones "mÃ¡gicas" de tu cÃ³digo.
    *   [ ] Crear un archivo `config.py` o un `.yaml` donde definas parÃ¡metros como el `k` de las recomendaciones, las rutas a los modelos guardados, etc. Esto evita tener nÃºmeros hardcodeados en el cÃ³digo.
*   [ ] **Pruebas (Testing):** Ampliar la cobertura de tests en `/tests`.
    *   [ ] Escribir tests para la API: simula llamadas a tus endpoints (`/recommend`, `/similar`) y verifica que devuelven un cÃ³digo de estado 200 y un JSON con el formato esperado. Usa `pytest` con `FastAPI.TestClient`.
    *   [ ] Escribir un test para el caso de "cold start" (un usuario que no existe).
*   [ ] **CI/CD:** Asegurarte de que tus nuevos tests se ejecutan automÃ¡ticamente en el workflow de GitHub Actions.
*   [ ] **API (Mejora):** Hacer que la respuesta de la API sea mÃ¡s Ãºtil. En lugar de solo devolver `movie_id`, haz que devuelva tambiÃ©n el `title` y los `genres` de la pelÃ­cula, realizando un "join" con el dataframe de pelÃ­culas.
*   [ ] **Retrospectiva Semanal:** Anotar: Â¿Por quÃ© es importante separar la configuraciÃ³n del cÃ³digo? Â¿QuÃ© fue lo mÃ¡s difÃ­cil de testear?

---

## ğŸ“Œ Semana 8: Limpieza Final y DocumentaciÃ³n del Proyecto

*   [ ] **DocumentaciÃ³n (README):** Realizar una actualizaciÃ³n masiva del `README.md`.
    *   [ ] AÃ±adir una secciÃ³n detallada de "MetodologÃ­a", explicando los diferentes modelos que probaste (colaborativo, contenido, hÃ­brido).
    *   [ ] Incluir una tabla con los resultados finales de cada modelo segÃºn tus mÃ©tricas (RMSE, Precision@k, etc.), sacados de MLflow.
    *   [ ] AÃ±adir una secciÃ³n de "Uso de la API" con ejemplos de cÃ³mo llamar a cada endpoint.
    *   [ ] (Bonus) Crear un GIF o un video corto mostrando cÃ³mo funciona la API (puedes usar la documentaciÃ³n interactiva de FastAPI en el navegador) e incrÃºstalo en el README.
*   [ ] **CÃ³digo:** Realizar una limpieza final del cÃ³digo.
    *   [ ] Eliminar todos los notebooks "de prueba" o limpiarlos para que solo quede el de EDA final.
    *   [ ] Revisar todo el cÃ³digo de `/src` para asegurar que sigue los estÃ¡ndares de estilo (puedes usar un linter como `flake8`).
    *   [ ] Asegurarse de que el `requirements.txt` estÃ© actualizado y no contenga librerÃ­as innecesarias.
*   [ ] **PresentaciÃ³n:** Preparar un borrador de la presentaciÃ³n del proyecto (como se discutiÃ³ en las recomendaciones). Â¿CÃ³mo le explicarÃ­as este proyecto a un reclutador en 5 minutos?
*   [ ] **Retrospectiva Mensual:** Evaluar el proyecto finalizado. Â¿Alcanzaste la mÃ©trica de `PrecisiÃ³n > 85%` (o la mÃ©trica que hayas definido como principal)? Â¿QuÃ© harÃ­as diferente si empezaras de nuevo?