# üèõÔ∏è Post-mortem T√©cnico: Fallo en el Registro de Artefactos de MLflow en un Entorno Dockerizado

**Etiquetas:** #mlops #docker #mlflow #post-mortem #debugging #docker-volumes #client-server

## Resumen Ejecutivo

Durante el despliegue de la pila de MLOps (`MLflow` + `FastAPI`) a trav√©s de `Docker Compose`, se encontr√≥ un fallo persistente y silencioso en el registro de artefactos del modelo. Aunque las m√©tricas y los par√°metros se registraban correctamente, los artefactos no aparec√≠an en la UI de MLflow. Un an√°lisis forense revel√≥ que la causa ra√≠z no era un √∫nico error, sino una cascada de problemas sist√©micos derivados de una arquitectura de seguimiento incorrecta para un entorno contenedorizado heterog√©neo (host Windows, contenedor Linux). La soluci√≥n requiri√≥ una refactorizaci√≥n de la arquitectura de seguimiento de un modelo de sistema de archivos compartido (`file://...`) a un modelo cliente-servidor puro (`http://...`), adem√°s de resolver incompatibilidades de versiones y condiciones de carrera en la inicializaci√≥n de servicios.

---

## 1. Descripci√≥n del Problema Inicial

- **S√≠ntoma Principal:** La ejecuci√≥n de `mlflow.pyfunc.log_model` desde un notebook de Jupyter se completaba sin lanzar excepciones. Sin embargo, la secci√≥n de "Artifacts" para el "run" correspondiente en la UI de MLflow permanec√≠a vac√≠a.
- **S√≠ntomas Secundarios:** En fases de depuraci√≥n anteriores, la UI de MLflow devolv√≠a errores gen√©ricos `500 Internal Server Error`, y los logs del contenedor revelaban errores `TypeError` relacionados con metadatos de "run" corruptos.

---

## 2. Metodolog√≠a de Diagn√≥stico y An√°lisis de Causa Ra√≠z (RCA)

El diagn√≥stico se realiz√≥ mediante la desarticulaci√≥n sistem√°tica del problema en tres fases, cada una revelando una capa m√°s profunda del fallo.

### Fase 1: Error de Conectividad de Red (`ConnectionRefusedError`)

- **An√°lisis:** La traza inicial de errores mostr√≥ una `ConnectionRefusedError` expl√≠cita al intentar ejecutar `mlflow.set_experiment()`. El cliente de MLflow no pod√≠a establecer una conexi√≥n TCP con el servidor en `localhost:5000`.
- **Causa Ra√≠z Identificada:** Violaci√≥n del orden de operaciones requerido para un sistema distribuido. El cliente (notebook) se ejecutaba antes de que la pila de `docker-compose` hubiera inicializado completamente y vinculado el puerto del servicio `mlflow-ui`.
- **Lecci√≥n T√©cnica:** La inicializaci√≥n de servicios en `docker-compose` no es instant√°nea. Un flujo de trabajo robusto debe garantizar que los servicios del backend est√©n en un estado `HEALTHY` o `LISTENING` antes de que los clientes intenten comunicarse con ellos.

### Fase 2: Corrupci√≥n de Estado y Conflicto de Versiones (`TypeError: missing run_uuid`)

- **An√°lisis:** Una vez corregido el orden de operaciones, la conexi√≥n se establec√≠a, pero la UI del servidor fallaba con un `500 Internal Server Error`. El `traceback` del contenedor `mlflow-ui` indicaba un `TypeError` fatal: `RunInfo.__init__() missing 1 required positional argument: 'run_uuid'`.
- **Causa Ra√≠z Identificada:** Incompatibilidad de esquema entre la versi√≥n de la librer√≠a de `mlflow` utilizada por el cliente (v3.x) y la versi√≥n de la imagen de Docker del servidor de `mlflow` (v2.x). La versi√≥n m√°s reciente del cliente serializaba los metadatos del "run" con un campo `run_id`, mientras que el servidor m√°s antiguo esperaba un campo `run_uuid`. Un solo archivo `meta.yaml` con el esquema incorrecto era suficiente para hacer que el `FileStore` del servidor fallara en su totalidad.
- **Lecci√≥n T√©cnica:** La paridad de versiones (o, como m√≠nimo, la compatibilidad garantizada) entre los componentes de cliente y servidor de una misma herramienta es un prerrequisito para la estabilidad del sistema.

### Fase 3: Falla de Sincronizaci√≥n de Volumen (`Bind Mount`)

- **An√°lisis:** Tras alinear las versiones y confirmar mediante inspecci√≥n manual del sistema de archivos (`ls -la`) que el notebook S√ç escrib√≠a los artefactos correctamente en el directorio `./mlruns` del host, la UI de MLflow segu√≠a sin mostrarlos.
- **Causa Ra√≠z Identificada:** Fallo en la capa de virtualizaci√≥n del sistema de archivos de Docker. El "bind mount" que mapea el directorio del host (`./mlruns`) al directorio del contenedor (`/mlruns`) no estaba sincronizando los archivos correctamente. Esto suele deberse a complejos problemas de permisos entre el sistema de archivos NTFS del host de Windows y el sistema de archivos POSIX del contenedor de Linux, especialmente en lo que respecta a la propiedad de los archivos (`UID`/`GID`).
- **Lecci√≥n T√©cnica:** Los "bind mounts" en entornos de desarrollo heterog√©neos (Windows-Linux) son inherentemente fr√°giles. Si bien son √∫tiles, introducen una fuerte dependencia del comportamiento y la configuraci√≥n de Docker Desktop.

---

## 3. Soluci√≥n Arquitect√≥nica Implementada

Se determin√≥ que la causa fundamental de todos los problemas era el **dise√±o arquitect√≥nico original**, que trataba al sistema de archivos local (`./mlruns`) como una base de datos compartida. Esto creaba un acoplamiento indebido y era la fuente de todos los conflictos. La soluci√≥n fue refactorizar a una **arquitectura cliente-servidor estricta**.

1.  **Reconfiguraci√≥n del Cliente (`tracking_uri`):** El notebook fue modificado para dejar de usar un `tracking_uri` basado en archivos (`file://...`). En su lugar, se configur√≥ para apuntar directamente al endpoint HTTP del servidor.
    ```python
    mlflow.set_tracking_uri("http://localhost:5000")
    ```
2.  **Aislamiento de la Responsabilidad de Escritura:** En esta nueva arquitectura, el cliente del notebook **nunca escribe directamente en `./mlruns`**. Su √∫nica responsabilidad es enviar los datos (par√°metros, m√©tricas y el payload binario de los artefactos) al servidor a trav√©s de la API REST de MLflow.
3.  **Centralizaci√≥n de la Persistencia:** El servidor de MLflow (corriendo dentro de Docker) se convierte en la **√∫nica entidad autorizada para escribir** en el sistema de archivos. Recibe los datos del cliente a trav√©s de la red y los persiste en su propio directorio `/mlruns`, que a su vez est√° mapeado de vuelta al host a trav√©s del `volume`.

---

## 4. Conclusi√≥n y Lecci√≥n de Ingenier√≠a Senior

La cascada de fallos fue el resultado directo de violar un principio de dise√±o clave para sistemas distribuidos: la **separaci√≥n de incumbencias (`Separation of Concerns`)**. Al tratar el sistema de archivos como una base de datos compartida, se cre√≥ una dependencia fr√°gil y compleja entre el host y el contenedor.

La transici√≥n a una arquitectura cliente-servidor pura, donde la comunicaci√≥n se produce exclusivamente a trav√©s de una API de red bien definida, elimina esta clase de problemas por dise√±o. El sistema resultante es m√°s robusto, predecible y menos dependiente de la configuraci√≥n del entorno del host, aline√°ndose con las mejores pr√°cticas para sistemas de MLOps en producci√≥n. El `Traceback` final (`ConnectionRefusedError` vs. `500 Internal Server Error`) fue la pieza de diagn√≥stico clave que permiti√≥ diferenciar entre un problema de disponibilidad de red y un problema de estado corrupto, guiando la soluci√≥n final.